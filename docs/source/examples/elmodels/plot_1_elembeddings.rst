
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/elmodels/plot_1_elembeddings.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_elmodels_plot_1_elembeddings.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_elmodels_plot_1_elembeddings.py:


EL Embeddings
===============

This example corresponds to the paper `EL Embeddings: Geometric Construction of Models for the Description Logic EL++ <https://www.ijcai.org/proceedings/2019/845>`_.

The idea of this paper is to embed EL by modeling ontology classes as :math:`n`-dimensional balls (:math:`n`-balls) and ontology object properties as transformations of those :math:`n`-balls. For each of the normal forms, there is a distance function defined that will work as loss functions in the optimization framework.

.. GENERATED FROM PYTHON SOURCE LINES 15-16

Let's just define the imports that will be needed along the example:

.. GENERATED FROM PYTHON SOURCE LINES 16-27

.. code-block:: default


    import mowl
    mowl.init_jvm("10g")
    from mowl.base_models.elmodel import EmbeddingELModel
    from mowl.models.elembeddings.evaluate import ELEmbeddingsPPIEvaluator
    from mowl.nn.elmodule import ELModule
    import numpy as np
    import torch as th
    from torch import nn
    from tqdm import trange








.. GENERATED FROM PYTHON SOURCE LINES 28-44

The EL-Embeddings model, maps ontology classes, object properties and operators into a
geometric model. The :math:`\mathcal{EL}` description logic is expressed using the
following General Concept Inclusions (GCIs):

.. math::
   \begin{align}
   C &\sqsubseteq D & (\text{GCI 0}) \\
   C_1 \sqcap C_2 &\sqsubseteq D & (\text{GCI 1}) \\
   C &\sqsubseteq \exists R. D & (\text{GCI 2})\\
   \exists R. C &\sqsubseteq D & (\text{GCI 3})\\
   C &\sqsubseteq \bot & (\text{GCI BOT 0}) \\
   C_1 \sqcap C_2 &\sqsubseteq \bot & (\text{GCI BOT 1}) \\
   \exists R. C &\sqsubseteq \bot & (\text{GCI BOT 3})
   \end{align}

where :math:`C,C_1, C_2,D` are ontology classes and :math:`R` is an ontology object property

.. GENERATED FROM PYTHON SOURCE LINES 46-53

EL-Embeddings uses GCI 0, 1, 2, 3 and GCI BOT 1 (to express disjointness between classes).
In the use case of this example, we will test over a biological problem, which is
protein-protein interactions. Given two proteins :math:`p_1,p_2`, the phenomenon
":math:`p_1` interacts with :math:`p_1`" is encoded using GCI 2 as:

.. math::
   p_1 \sqsubseteq interacts\_with. p_2

.. GENERATED FROM PYTHON SOURCE LINES 55-61

Definition of the model and the loss functions
---------------------------------------------------

In this part we define the neural network part. As mentioned earlier, ontology classes \
are :math:`n`-dimensional balls. Each ball has a center :math:`c \in \mathbb{R}^n` and \
radius :math:`r \in \mathbb{R}`. :math:`n` will be the embedding size.

.. GENERATED FROM PYTHON SOURCE LINES 62-185

.. code-block:: default



    class ELEmModule(ELModule):

        def __init__(self, nb_ont_classes, nb_rels, embed_dim=50, margin=0.1):
            super().__init__()
            self.nb_ont_classes = nb_ont_classes
            self.nb_rels = nb_rels

            self.embed_dim = embed_dim

            # Embedding layer for classes centers.
            self.class_embed = nn.Embedding(self.nb_ont_classes, embed_dim)
            nn.init.uniform_(self.class_embed.weight, a=-1, b=1)
            weight_data = th.linalg.norm(self.class_embed.weight.data, axis=1).reshape(-1, 1)
            self.class_embed.weight.data /= weight_data

            # Embedding layer for classes radii.
            self.class_rad = nn.Embedding(self.nb_ont_classes, 1)
            nn.init.uniform_(self.class_rad.weight, a=-1, b=1)
            weight_data = th.linalg.norm(self.class_rad.weight.data, axis=1).reshape(-1, 1)
            self.class_rad.weight.data /= weight_data

            # Embedding layer for ontology object properties
            self.rel_embed = nn.Embedding(nb_rels, embed_dim)
            nn.init.uniform_(self.rel_embed.weight, a=-1, b=1)
            weight_data = th.linalg.norm(self.rel_embed.weight.data, axis=1).reshape(-1, 1)
            self.rel_embed.weight.data /= weight_data

            self.margin = margin

        # Regularization method to force n-ball to be inside unit ball
        def class_reg(self, x):
            res = th.abs(th.linalg.norm(x, axis=1) - 1)
            res = th.reshape(res, [-1, 1])
            return res

        # Loss function for normal form :math:`C \sqsubseteq D`
        def gci0_loss(self, data, neg=False):
            c = self.class_embed(data[:, 0])
            d = self.class_embed(data[:, 1])
            rc = th.abs(self.class_rad(data[:, 0]))
            rd = th.abs(self.class_rad(data[:, 1]))
            dist = th.linalg.norm(c - d, dim=1, keepdim=True) + rc - rd
            loss = th.relu(dist - self.margin)
            return loss + self.class_reg(c) + self.class_reg(d)

        # Loss function for normal form :math:`C \sqcap D \sqsubseteq E`
        def gci1_loss(self, data, neg=False):
            c = self.class_embed(data[:, 0])
            d = self.class_embed(data[:, 1])
            e = self.class_embed(data[:, 2])
            rc = th.abs(self.class_rad(data[:, 0]))
            rd = th.abs(self.class_rad(data[:, 1]))

            sr = rc + rd
            dst = th.linalg.norm(d - c, dim=1, keepdim=True)
            dst2 = th.linalg.norm(e - c, dim=1, keepdim=True)
            dst3 = th.linalg.norm(e - d, dim=1, keepdim=True)
            loss = th.relu(dst - sr - self.margin) + th.relu(dst2 - rc - self.margin)
            loss += th.relu(dst3 - rd - self.margin)

            return loss + self.class_reg(c) + self.class_reg(d) + self.class_reg(e)

        # Loss function for normal form :math:`C \sqcap D \sqsubseteq \bot`
        def gci1_bot_loss(self, data, neg=False):
            c = self.class_embed(data[:, 0])
            d = self.class_embed(data[:, 1])
            rc = self.class_rad(data[:, 0])
            rd = self.class_rad(data[:, 1])

            sr = rc + rd
            dst = th.reshape(th.linalg.norm(d - c, axis=1), [-1, 1])
            return th.relu(sr - dst + self.margin) + self.class_reg(c) + self.class_reg(d)

        # Loss function for normal form :math:`C \sqsubseteq \exists R. D`
        def gci2_loss(self, data, neg=False):

            if neg:
                return self.gci2_loss_neg(data)

            else:
                # C subSelf.ClassOf R some D
                c = self.class_embed(data[:, 0])
                rE = self.rel_embed(data[:, 1])
                d = self.class_embed(data[:, 2])

                rc = th.abs(self.class_rad(data[:, 0]))
                rd = th.abs(self.class_rad(data[:, 2]))

                dst = th.linalg.norm(c + rE - d, dim=1, keepdim=True)
                loss = th.relu(dst + rc - rd - self.margin)
                return loss + self.class_reg(c) + self.class_reg(d)

        # Loss function for normal form :math:`C \nsqsubseteq \exists R. D`
        def gci2_loss_neg(self, data):

            c = self.class_embed(data[:, 0])
            rE = self.rel_embed(data[:, 1])

            d = self.class_embed(data[:, 2])
            rc = th.abs(self.class_rad(data[:, 1]))
            rd = th.abs(self.class_rad(data[:, 2]))

            dst = th.linalg.norm(c + rE - d, dim=1, keepdim=True)
            loss = th.relu(rc + rd - dst + self.margin)
            return loss + self.class_reg(c) + self.class_reg(d)

        # Loss function for normal form :math:`\exists R. C \sqsubseteq D`
        def gci3_loss(self, data, neg=False):

            rE = self.rel_embed(data[:, 0])
            c = self.class_embed(data[:, 1])
            d = self.class_embed(data[:, 2])
            rc = th.abs(self.class_rad(data[:, 1]))
            rd = th.abs(self.class_rad(data[:, 2]))

            euc = th.linalg.norm(c - rE - d, dim=1, keepdim=True)
            loss = th.relu(euc - rc - rd - self.margin)
            return loss + self.class_reg(c) + self.class_reg(d)










.. GENERATED FROM PYTHON SOURCE LINES 186-189

Now, let's first write the code containing the tranining and validation parts.
For that, let's use the
:class:`EmbeddingELModel <mowl.base_models.elmodel.EmbeddingELModel>` class.

.. GENERATED FROM PYTHON SOURCE LINES 189-285

.. code-block:: default


    class ELEmbeddings(EmbeddingELModel):

        def __init__(self,
                     dataset,
                     embed_dim=50,
                     margin=0,
                     reg_norm=1,
                     learning_rate=0.001,
                     epochs=1000,
                     batch_size=4096 * 8,
                     model_filepath=None,
                     device='cpu'
                     ):
            super().__init__(dataset, batch_size, extended=True, model_filepath=model_filepath)

            self.embed_dim = embed_dim
            self.margin = margin
            self.reg_norm = reg_norm
            self.learning_rate = learning_rate
            self.epochs = epochs
            self.device = device
            self._loaded = False
            self._loaded_eval = False
            self.extended = False
            self.init_model()
            
        def init_model(self):
            self.model = ELEmModule(
                len(self.class_index_dict),  # number of ontology classes
                len(self.object_property_index_dict),  # number of ontology object properties
                embed_dim=self.embed_dim,
                margin=self.margin
            ).to(self.device)

        def train(self):
            optimizer = th.optim.Adam(self.model.parameters(), lr=self.learning_rate)
            best_loss = float('inf')

            for epoch in trange(self.epochs):
                self.model.train()

                train_loss = 0
                loss = 0

                # Notice how we use the ``training_datasets`` variable directly
                # and every element of it is a pair (GCI name, GCI tensor data).
                for gci_name, gci_dataset in self.training_datasets.items():
                    if len(gci_dataset) == 0:
                        continue

                    loss += th.mean(self.model(gci_dataset[:], gci_name))
                    if gci_name == "gci2":
                        prots = [self.class_index_dict[p] for p in self.dataset.evaluation_classes.as_str]
                        idxs_for_negs = np.random.choice(prots, size=len(gci_dataset), replace=True)
                        rand_index = th.tensor(idxs_for_negs).to(self.device)
                        data = gci_dataset[:]
                        neg_data = th.cat([data[:, :2], rand_index.unsqueeze(1)], dim=1)
                        loss += th.mean(self.model(neg_data, gci_name, neg=True))

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                train_loss += loss.detach().item()

                loss = 0
                with th.no_grad():
                    self.model.eval()
                    valid_loss = 0
                    gci2_data = self.validation_datasets["gci2"][:]
                    loss = th.mean(self.model(gci2_data, "gci2"))
                    valid_loss += loss.detach().item()

                checkpoint = 1
                if best_loss > valid_loss:
                    best_loss = valid_loss
                    th.save(self.model.state_dict(), self.model_filepath)
                if (epoch + 1) % checkpoint == 0:
                    print(f'\nEpoch {epoch}: Train loss: {train_loss:4f} Valid loss: {valid_loss:.4f}')

        def evaluate_ppi(self):
            self.init_model()
            print('Load the best model', self.model_filepath)
            self.model.load_state_dict(th.load(self.model_filepath))
            with th.no_grad():
                self.model.eval()

                eval_method = self.model.gci2_loss

                evaluator = ELEmbeddingsPPIEvaluator(
                    self.dataset.testing, eval_method, self.dataset.ontology,
                    self.class_index_dict, self.object_property_index_dict, device=self.device)
                evaluator()
                evaluator.print_metrics()









.. GENERATED FROM PYTHON SOURCE LINES 286-288

Training the model
-------------------

.. GENERATED FROM PYTHON SOURCE LINES 288-305

.. code-block:: default



    from mowl.datasets.builtin import PPIYeastSlimDataset

    dataset = PPIYeastSlimDataset()

    model = ELEmbeddings(dataset,
                         embed_dim=10,
                         margin=0.1,
                         reg_norm=1,
                         learning_rate=0.001,
                         epochs=20,
                         batch_size=4096,
                         model_filepath=None,
                         device='cpu')

    model.train()




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/20 [00:00<?, ?it/s]
    Epoch 0: Train loss: 3.379612 Valid loss: 1.6100
      5%|5         | 1/20 [00:11<03:35, 11.32s/it]
    Epoch 1: Train loss: 3.388901 Valid loss: 1.6072

    Epoch 2: Train loss: 3.371835 Valid loss: 1.6055
     15%|#5        | 3/20 [00:11<00:50,  2.99s/it]
    Epoch 3: Train loss: 3.360057 Valid loss: 1.6032

    Epoch 4: Train loss: 3.344359 Valid loss: 1.6011
     25%|##5       | 5/20 [00:11<00:22,  1.49s/it]
    Epoch 5: Train loss: 3.336682 Valid loss: 1.5988

    Epoch 6: Train loss: 3.326580 Valid loss: 1.5972
     35%|###5      | 7/20 [00:11<00:11,  1.12it/s]
    Epoch 7: Train loss: 3.316017 Valid loss: 1.5958

    Epoch 8: Train loss: 3.304600 Valid loss: 1.5941
     45%|####5     | 9/20 [00:11<00:06,  1.70it/s]
    Epoch 9: Train loss: 3.293371 Valid loss: 1.5928

    Epoch 10: Train loss: 3.281227 Valid loss: 1.5915
     55%|#####5    | 11/20 [00:12<00:03,  2.45it/s]
    Epoch 11: Train loss: 3.274224 Valid loss: 1.5899

    Epoch 12: Train loss: 3.264461 Valid loss: 1.5882
     65%|######5   | 13/20 [00:12<00:02,  3.35it/s]
    Epoch 13: Train loss: 3.256842 Valid loss: 1.5866

    Epoch 14: Train loss: 3.250233 Valid loss: 1.5849
     75%|#######5  | 15/20 [00:12<00:01,  4.41it/s]
    Epoch 15: Train loss: 3.245277 Valid loss: 1.5833

    Epoch 16: Train loss: 3.239470 Valid loss: 1.5816
     85%|########5 | 17/20 [00:12<00:00,  5.58it/s]
    Epoch 17: Train loss: 3.232712 Valid loss: 1.5801

    Epoch 18: Train loss: 3.226809 Valid loss: 1.5785
     95%|#########5| 19/20 [00:12<00:00,  6.79it/s]
    Epoch 19: Train loss: 3.220435 Valid loss: 1.5770
    100%|##########| 20/20 [00:12<00:00,  1.56it/s]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  17.188 seconds)


.. _sphx_glr_download_examples_elmodels_plot_1_elembeddings.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_1_elembeddings.py <plot_1_elembeddings.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_1_elembeddings.ipynb <plot_1_elembeddings.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
